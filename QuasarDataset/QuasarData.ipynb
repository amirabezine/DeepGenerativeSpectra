{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "555d3c67-6305-4fb4-9dbc-a94f8aaa1b81",
   "metadata": {},
   "source": [
    "# Data Implementation For Quasars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713a9dd-51c1-4bf2-8aaf-67d466d2e898",
   "metadata": {},
   "source": [
    "## Part I - To find QSO in database and create the txt file (not necessary if created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0852f9c-2668-4b8b-8f03-654345c8cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query the SDSS database to find QSO\n",
    "query_result = SDSS.query_sql(\"\"\"\n",
    "    SELECT\n",
    "        plate, mjd, fiberid, z\n",
    "    FROM\n",
    "        specObj\n",
    "    WHERE\n",
    "        class = 'QSO'\n",
    "\"\"\")\n",
    "\n",
    "# Print the first few rows of the query result\n",
    "print(query_result[:5])\n",
    "#Print length of result found\n",
    "print(\"\\nNb quasars in SDSS database :\",len(query_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a73a32-7d12-4fde-a212-8b5a398a4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING : TO LAUNCH ONLY IF THE \"qso.txt\" FILE IS TO BE UPDATED\n",
    "#VERY LONG\n",
    "directory = '/arc/projects/k-pop/spectra/boss/lite'\n",
    "qso_files = []\n",
    "redshift = []\n",
    "wavelength = []\n",
    "flux = []\n",
    "ivar = []\n",
    "mask = []\n",
    "\n",
    "# Indexer files by a reducted format name\n",
    "file_index = {}\n",
    "for dossier_parent, _, fichiers in os.walk(directory):\n",
    "    for nom_fichier in fichiers:\n",
    "        if nom_fichier.startswith(\"spec-\") and nom_fichier.endswith(\".fits\"):\n",
    "            # Extraire le numéro de plaque, de MJD et de fibre du nom de fichier\n",
    "            parts = nom_fichier.split('-')\n",
    "            plate = parts[1]\n",
    "            mjd = parts[2]\n",
    "            fiberid = parts[3].split('.')[0]  # Retirer l'extension .fits\n",
    "            # Forcer fiberid à être représenté par quatre chiffres\n",
    "            fiberid = fiberid.zfill(4)\n",
    "            file_index[f\"spec-{plate}-{mjd}-{fiberid}.fits\"] = os.path.join(dossier_parent, nom_fichier)\n",
    "\n",
    "# Processus for each QSO\n",
    "for qso in tqdm(qso_list, desc=\"Processing items\"):\n",
    "    plate, mjd, fiberid = qso['plate'], qso['mjd'], qso['fiberid']\n",
    "    # Force fiberid to have for digits\n",
    "    nom_fichier = f\"spec-{plate}-{mjd}-{fiberid:04d}.fits\"\n",
    "    if nom_fichier in file_index:\n",
    "        filepath = file_index[nom_fichier]\n",
    "        qso_files.append(filepath)\n",
    "        redshift.append(qso['z'])\n",
    "        # Lire le fichier FITS\n",
    "        with fits.open(filepath) as hdul:\n",
    "            data = hdul[1].data\n",
    "\n",
    "        # Extract necessary columns\n",
    "        wavelength.append(np.power([10]*len(data['loglam']), data['loglam']))\n",
    "        flux.append(data['FLUX'])\n",
    "        ivar.append(data['IVAR'])\n",
    "        mask.append(data['AND_MASK'])\n",
    "\n",
    "print(\"Nb quasars in our database :\", len(qso_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ac7f20-f596-43e0-8152-40f13b3e9f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to place qso_files in qso.txt\n",
    "# path to the txt file\n",
    "chemin_fichier_texte = \"/arc/home/amolinard/qso.txt\"\n",
    "\n",
    "with open(chemin_fichier_texte, 'w') as f:\n",
    "    for qso in qso_files:\n",
    "        f.write(qso + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45c26c9-135b-4342-9493-4fa645c1b7ed",
   "metadata": {},
   "source": [
    "## Part 2 - Creation of the HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3146578e-eb7c-4a75-af4d-c4395f44fb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting FITS to HDF5:   0%|          | 0/5371 [00:00<?, ?it/s]/tmp/ipykernel_1181/2380434420.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  mask = np.where((flux == 0) | (1/ivar > 0.5), 0, 1)\n",
      "Converting FITS to HDF5: 100%|██████████| 5371/5371 [1:43:54<00:00,  1.16s/it]  \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os as os\n",
    "from tqdm import tqdm \n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "max_length = 4621\n",
    "\n",
    "#From fits to hdf5\n",
    "def read_txt_file(path_file):\n",
    "    with open(path_file, 'r') as f:\n",
    "        path = f.read().splitlines()\n",
    "    return path\n",
    "\n",
    "def ensure_native_byteorder(array):\n",
    "    if array.dtype.byteorder not in ('=', '|'):  # '=' means native, '|' means not applicable\n",
    "        return array.byteswap().newbyteorder()  # Swap byte order to native\n",
    "    return array\n",
    "\n",
    "def create_mask(flux, ivar):\n",
    "    \"\"\"\n",
    "    Creates a mask for the flux array where the mask is 0 if the flux is zero or sigma > 0.5, and 1 otherwise.\n",
    "    \"\"\"\n",
    "    mask = np.where((flux == 0) | (1/ivar > 0.5), 0, 1)\n",
    "    return mask\n",
    "\n",
    "def convert_fits_to_hdf5(hdf5_path, max_files=300, save_interval=10):\n",
    "    count = 0\n",
    "    with h5py.File(hdf5_path, 'w') as hdf5_file:\n",
    "        all_files = read_txt_file('/arc/home/amolinard/qso.txt')\n",
    "        all_files = all_files[:max_files]  # Limit the number of files\n",
    "        for i in tqdm(range(0, len(all_files), save_interval), desc=\"Converting FITS to HDF5\"):\n",
    "            for file_name in all_files[i:i+save_interval]:\n",
    "                file_path = file_name\n",
    "                with fits.open(file_path) as hdul:\n",
    "                    spectrum_index = count\n",
    "                    count = count + 1\n",
    "                    data = hdul[1].data\n",
    "                    add_data = hdul['SPALL'].data\n",
    "                    redshift = add_data['Z'][0]\n",
    "                    flux = data['FLUX']\n",
    "                    ivar = data['IVAR']\n",
    "                    length = len(flux)\n",
    "                    wavelength = np.power([10]*len(data['loglam']),data['loglam'])\n",
    "\n",
    "                    \n",
    "                    #To have data of the same dimension\n",
    "                    padded_flux = np.pad(flux, (0, max_length-len(flux)), mode='constant', constant_values=0)\n",
    "                    padded_ivar = np.pad(ivar, (0, max_length-len(ivar)), mode='constant', constant_values=0)\n",
    "                    padded_wavelength = np.pad(wavelength, (0, max_length-len(wavelength)), mode='constant', constant_values=0)\n",
    "\n",
    "                    flux = ensure_native_byteorder(padded_flux)\n",
    "                    ivar = ensure_native_byteorder(padded_ivar)\n",
    "                    wavelength = ensure_native_byteorder(padded_wavelength)\n",
    "                    \n",
    "                    flux_mask = create_mask(padded_flux, padded_ivar).astype(np.float32)\n",
    "\n",
    "                    short_file_name = os.path.basename(file_name)\n",
    "                    grp = hdf5_file.create_group(short_file_name)\n",
    "                    grp.create_dataset('flux', data=padded_flux)\n",
    "                    grp.create_dataset('wavelength', data=padded_wavelength)\n",
    "                    grp.create_dataset('mask', data=flux_mask)\n",
    "                    grp.create_dataset('ivar', data=padded_ivar)\n",
    "                    grp.create_dataset('redshift', data=redshift)\n",
    "                    grp.create_dataset('length', data=length)\n",
    "                    grp.create_dataset('spectrum_index', data=spectrum_index)\n",
    "            \n",
    "            hdf5_file.flush()  # Ensure data is written to disk\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_fits_to_hdf5('/arc/home/amolinard/Padded_Database.hdf5',60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54004225-5a3b-4eab-a743-f64bcc362870",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting FITS to HDF5:   0%|          | 0/5371 [00:00<?, ?it/s]/tmp/ipykernel_1181/4164100400.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  mask = np.where((flux == 0) | (1/ivar > 0.5), 0, 1)\n",
      "Converting FITS to HDF5: 100%|██████████| 5371/5371 [2:40:56<00:00,  1.80s/it]  \n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import os as os\n",
    "from tqdm import tqdm \n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "\n",
    "max_length = 4621\n",
    "\n",
    "#From fits to hdf5\n",
    "def read_txt_file(path_file):\n",
    "    with open(path_file, 'r') as f:\n",
    "        path = f.read().splitlines()\n",
    "    return path\n",
    "\n",
    "def ensure_native_byteorder(array):\n",
    "    if array.dtype.byteorder not in ('=', '|'):  # '=' means native, '|' means not applicable\n",
    "        return array.byteswap().newbyteorder()  # Swap byte order to native\n",
    "    return array\n",
    "\n",
    "def create_mask(flux, ivar):\n",
    "    \"\"\"\n",
    "    Creates a mask for the flux array where the mask is 0 if the flux is zero or sigma > 0.5, and 1 otherwise.\n",
    "    \"\"\"\n",
    "    mask = np.where((flux == 0) | (1/ivar > 0.5), 0, 1)\n",
    "    return mask\n",
    "\n",
    "def convert_fits_to_hdf5(hdf5_path, max_files=300, save_interval=10):\n",
    "    count = 0\n",
    "    with h5py.File(hdf5_path, 'w') as hdf5_file:\n",
    "        all_files = read_txt_file('/arc/home/amolinard/qso.txt')\n",
    "        all_files = all_files[:max_files]  # Limit the number of files\n",
    "        file_names = hdf5_file.create_dataset('file_names', (max_files,), dtype=h5py.string_dtype())\n",
    "        for i in tqdm(range(0, len(all_files), save_interval), desc=\"Converting FITS to HDF5\"):\n",
    "            for file_name in all_files[i:i+save_interval]:\n",
    "                file_path = file_name\n",
    "                with fits.open(file_path) as hdul:\n",
    "                    spectrum_index = count\n",
    "                    count = count + 1\n",
    "                    data = hdul[1].data\n",
    "                    add_data = hdul['SPALL'].data\n",
    "                    redshift = add_data['Z'][0]\n",
    "                    flux = data['FLUX']\n",
    "                    ivar = data['IVAR']\n",
    "                    length = len(flux)\n",
    "                    wavelength = np.power(10, data['loglam'])\n",
    "                    \n",
    "                    #To have data of the same dimension\n",
    "                    padded_flux = np.pad(flux, (0, max_length-len(flux)), mode='constant', constant_values=0)\n",
    "                    padded_ivar = np.pad(ivar, (0, max_length-len(ivar)), mode='constant', constant_values=0)\n",
    "                    padded_wavelength = np.pad(wavelength, (0, max_length-len(wavelength)), mode='constant', constant_values=0)\n",
    "\n",
    "                    flux = ensure_native_byteorder(padded_flux)\n",
    "                    ivar = ensure_native_byteorder(padded_ivar)\n",
    "                    wavelength = ensure_native_byteorder(padded_wavelength)\n",
    "                    \n",
    "                    flux_mask = create_mask(padded_flux, padded_ivar).astype(np.float32)\n",
    "\n",
    "                    short_file_name = os.path.basename(file_name)\n",
    "                    file_names[spectrum_index] = short_file_name\n",
    "                    grp = hdf5_file.create_group(short_file_name)\n",
    "                    grp.create_dataset('flux', data=padded_flux)\n",
    "                    grp.create_dataset('wavelength', data=padded_wavelength)\n",
    "                    grp.create_dataset('mask', data=flux_mask)\n",
    "                    grp.create_dataset('ivar', data=padded_ivar)\n",
    "                    grp.create_dataset('redshift', data=redshift)\n",
    "                    grp.create_dataset('length', data=length)\n",
    "                    grp.create_dataset('spectrum_index', data=spectrum_index)\n",
    "            \n",
    "            hdf5_file.flush()  # Ensure data is written to disk\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_fits_to_hdf5('/arc/home/amolinard/Padded_Database.hdf5',60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547cd60-9638-4b42-a2e9-446433c50532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
